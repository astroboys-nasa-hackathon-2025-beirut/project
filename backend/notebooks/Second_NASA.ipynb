{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o057vU6GFEG",
        "outputId": "9c72bf30-3169-4331-dd39-cb28aafa5e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backup saved to: TESS_original_backup.csv\n",
            "Columns count: 87\n",
            "['rowid', 'toi', 'toipfx', 'tid', 'ctoi_alias', 'pl_pnum', 'tfopwg_disp', 'rastr', 'ra', 'raerr1', 'raerr2', 'decstr', 'dec', 'decerr1', 'decerr2', 'st_pmra', 'st_pmraerr1', 'st_pmraerr2', 'st_pmralim', 'st_pmrasymerr', 'st_pmdec', 'st_pmdecerr1', 'st_pmdecerr2', 'st_pmdeclim', 'st_pmdecsymerr', 'pl_tranmid', 'pl_tranmiderr1', 'pl_tranmiderr2', 'pl_tranmidlim', 'pl_tranmidsymerr', 'pl_orbper', 'pl_orbpererr1', 'pl_orbpererr2', 'pl_orbperlim', 'pl_orbpersymerr', 'pl_trandurh', 'pl_trandurherr1', 'pl_trandurherr2', 'pl_trandurhlim', 'pl_trandurhsymerr', 'pl_trandep', 'pl_trandeperr1', 'pl_trandeperr2', 'pl_trandeplim', 'pl_trandepsymerr', 'pl_rade', 'pl_radeerr1', 'pl_radeerr2', 'pl_radelim', 'pl_radesymerr', 'pl_insol', 'pl_insolerr1', 'pl_insolerr2', 'pl_insollim', 'pl_insolsymerr', 'pl_eqt', 'pl_eqterr1', 'pl_eqterr2', 'pl_eqtlim', 'pl_eqtsymerr', 'st_tmag', 'st_tmagerr1', 'st_tmagerr2', 'st_tmaglim', 'st_tmagsymerr', 'st_dist', 'st_disterr1', 'st_disterr2', 'st_distlim', 'st_distsymerr', 'st_teff', 'st_tefferr1', 'st_tefferr2', 'st_tefflim', 'st_teffsymerr', 'st_logg', 'st_loggerr1', 'st_loggerr2', 'st_logglim', 'st_loggsymerr', 'st_rad', 'st_raderr1', 'st_raderr2', 'st_radlim', 'st_radsymerr', 'toi_created', 'rowupdate']\n",
            "Present keep columns: ['tfopwg_disp', 'ra', 'dec', 'st_pmra', 'st_pmdec', 'pl_tranmid', 'pl_orbper', 'pl_trandurh', 'pl_trandep', 'pl_rade', 'pl_insol', 'pl_eqt', 'st_tmag', 'st_dist', 'st_teff', 'st_logg', 'st_rad']\n",
            "Missing expected columns (investigate): []\n",
            "============================================================\n",
            "TARGET MAPPING\n",
            "============================================================\n",
            "Mapped target distribution:\n",
            "tfopwg_disp_mapped\n",
            "Planet Candidate    4679\n",
            "Confirmed Planet    1267\n",
            "False Positive      1197\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label mapping:\n",
            "  Confirmed Planet   -> 0 (1267 samples)\n",
            "  Planet Candidate   -> 1 (4679 samples)\n",
            "  False Positive     -> 2 (1197 samples)\n",
            "Intermediate cleaned file written to 'TESS_cleaned_for_review.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import difflib\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Read robustly ---\n",
        "df = pd.read_csv('TESS.csv', comment='#', sep=',', skip_blank_lines=True)\n",
        "df.columns = df.columns.str.strip()   # remove stray whitespace\n",
        "\n",
        "# Save a full backup\n",
        "backup_path = Path('TESS_original_backup.csv')\n",
        "df.to_csv(backup_path, index=False)\n",
        "print(\"Backup saved to:\", backup_path)\n",
        "\n",
        "# --- 2. Show columns and quick diagnostics ---\n",
        "print(\"Columns count:\", len(df.columns))\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# --- 3. Safe keep selection (intersection) ---\n",
        "keep_columns = [\n",
        "    'tfopwg_disp', 'ra', 'dec', 'st_pmra', 'st_pmdec',\n",
        "    'pl_tranmid','pl_orbper','pl_trandurh','pl_trandep','pl_rade',\n",
        "    'pl_insol','pl_eqt','st_tmag','st_dist','st_teff','st_logg','st_rad'\n",
        "]\n",
        "\n",
        "present = [c for c in keep_columns if c in df.columns]\n",
        "missing = list(set(keep_columns) - set(present))\n",
        "print(\"Present keep columns:\", present)\n",
        "print(\"Missing expected columns (investigate):\", missing)\n",
        "\n",
        "for m in missing:\n",
        "    print(m, \"-> close matches:\", difflib.get_close_matches(m, df.columns, n=6, cutoff=0.5))\n",
        "\n",
        "# Working copy\n",
        "df_work = df.copy()\n",
        "\n",
        "# --- 4. TARGET MAPPING & CLEANING ---\n",
        "print(\"=\"*60)\n",
        "print(\"TARGET MAPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Drop FA entirely\n",
        "df_work = df_work[~df_work['tfopwg_disp'].isin(['FA'])]\n",
        "\n",
        "# Map remaining classes to our 3-class scheme\n",
        "target_mapping = {\n",
        "    'CP': 'Confirmed Planet',\n",
        "    'KP': 'Confirmed Planet',\n",
        "    'PC': 'Planet Candidate',\n",
        "    'FP': 'False Positive'\n",
        "}\n",
        "\n",
        "df_work['tfopwg_disp_mapped'] = df_work['tfopwg_disp'].map(target_mapping)\n",
        "\n",
        "# Drop any rows that didn't match mapping (should not happen now)\n",
        "df_work = df_work[df_work['tfopwg_disp_mapped'].notna()]\n",
        "\n",
        "# Encode labels: 0=Confirmed, 1=Candidate, 2=False Positive\n",
        "label_map = {\n",
        "    'Confirmed Planet': 0,\n",
        "    'Planet Candidate': 1,\n",
        "    'False Positive': 2\n",
        "}\n",
        "df_work['label'] = df_work['tfopwg_disp_mapped'].map(label_map)\n",
        "\n",
        "# Summary\n",
        "print(\"Mapped target distribution:\")\n",
        "print(df_work['tfopwg_disp_mapped'].value_counts())\n",
        "print(\"\\nLabel mapping:\")\n",
        "for k, v in label_map.items():\n",
        "    count = (df_work['label'] == v).sum()\n",
        "    print(f\"  {k:18s} -> {v} ({count} samples)\")\n",
        "\n",
        "# --- 5. Save intermediate cleaned dataset ---\n",
        "df_work.to_csv('TESS_cleaned_for_review.csv', index=False)\n",
        "print(\"Intermediate cleaned file written to 'TESS_cleaned_for_review.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MISSING VALUE TREATMENT & FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MISSING VALUE TREATMENT & FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X = df_work.copy()  # operate on working copy\n",
        "\n",
        "# 1️⃣ Boolean flags: if any '_is_limit' columns exist, fill missing with 0\n",
        "bool_cols = [c for c in X.columns if '_is_limit' in c]\n",
        "for col in bool_cols:\n",
        "    before = X[col].isnull().sum()\n",
        "    X[col] = X[col].fillna(0).astype(int)\n",
        "    print(f\"Filled {col}: {before} missing -> 0\")\n",
        "\n",
        "# 2️⃣ Relative errors: fill missing with median\n",
        "err_cols = [c for c in X.columns if '_err_rel' in c]\n",
        "for col in err_cols:\n",
        "    if X[col].isnull().sum() > 0:\n",
        "        median_val = X[col].median()\n",
        "        before = X[col].isnull().sum()\n",
        "        X[col] = X[col].fillna(median_val)\n",
        "        print(f\"Filled {col}: {before} missing -> median {median_val:.6f}\")\n",
        "\n",
        "# 3️⃣ Core numeric features: fill missing with median (prevent dataset shrinkage)\n",
        "core_features = [\n",
        "    'pl_orbper', 'pl_trandurh', 'pl_trandep', 'pl_rade',\n",
        "    'st_teff', 'st_rad', 'st_logg', 'st_tmag'\n",
        "]\n",
        "for col in core_features:\n",
        "    if col in X.columns and X[col].isnull().sum() > 0:\n",
        "        median_val = X[col].median()\n",
        "        before = X[col].isnull().sum()\n",
        "        X[col] = X[col].fillna(median_val)\n",
        "        print(f\"Filled core feature {col}: {before} missing -> median {median_val:.4g}\")\n",
        "\n",
        "# 4️⃣ Replace infinities with NaN\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# 5️⃣ Fill remaining numeric NaNs with median (only numeric columns)\n",
        "numeric_cols = X.select_dtypes(include=np.number).columns\n",
        "if X[numeric_cols].isnull().sum().sum() > 0:\n",
        "    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
        "    print(f\"Filled remaining {X[numeric_cols].isnull().sum().sum()} numeric NaNs with median\")\n",
        "\n",
        "print(f\"\\nFinal missing values after imputation: {X.isnull().sum().sum()}\")\n",
        "\n",
        "# --- FEATURE ENGINEERING ---\n",
        "X['transit_signal_strength'] = X['pl_trandep'] * X['pl_trandurh']\n",
        "X['planet_star_radius_ratio'] = np.sqrt(X['pl_trandep'] / 1e6)\n",
        "X['stellar_density_proxy'] = (X['pl_trandurh'] / X['pl_orbper']) ** 2\n",
        "\n",
        "if 'pl_insol' in X.columns and 'pl_eqt' in X.columns:\n",
        "    X['insol_temp_ratio'] = X['pl_eqt'] / np.sqrt(X['pl_insol'])\n",
        "\n",
        "err_cols_present = [c for c in err_cols if c in X.columns]\n",
        "if err_cols_present:\n",
        "    X['avg_measurement_error'] = X[err_cols_present].mean(axis=1)\n",
        "\n",
        "if 'pl_eqt' in X.columns:\n",
        "    X['is_hot_jupiter'] = (X['pl_eqt'] > 1000).astype(int)\n",
        "\n",
        "X['is_small_planet'] = (X['pl_rade'] < 2.0).astype(int)\n",
        "X['is_bright_star'] = (X['st_tmag'] < 12.0).astype(int)\n",
        "\n",
        "if 'st_pmra' in X.columns and 'st_pmdec' in X.columns:\n",
        "    X['proper_motion_total'] = np.sqrt(X['st_pmra']**2 + X['st_pmdec']**2)\n",
        "\n",
        "X['log_period'] = np.log10(X['pl_orbper'])\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final feature count: {X.shape[1]} features\")\n",
        "print(f\"Sample count: {X.shape[0]} samples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATA SPLITTING & SCALING\n",
        "# ============================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DATA SPLITTING & SCALING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- 1️⃣ Define features and target ---\n",
        "y = X['label']\n",
        "X = X.drop(columns=['tfopwg_disp', 'tfopwg_disp_mapped', 'label'], errors='ignore')\n",
        "\n",
        "# --- 2️⃣ Split into train/test ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Train samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:  {X_test.shape[0]}\")\n",
        "\n",
        "# --- 3️⃣ Scale numeric columns ---\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit only on training numeric columns\n",
        "X_train_num = X_train.select_dtypes(include=np.number)\n",
        "X_test_num = X_test.select_dtypes(include=np.number)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_num)\n",
        "X_test_scaled = scaler.transform(X_test_num)\n",
        "\n",
        "# Convert back to DataFrame with same column names\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_num.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_num.columns, index=X_test.index)\n",
        "\n",
        "print(\"Scaling complete — numeric features standardized.\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNXLDYmarNTg",
        "outputId": "e88b66ec-09a7-4c9e-9a6c-bd83933c9f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MISSING VALUE TREATMENT & FEATURE ENGINEERING\n",
            "============================================================\n",
            "Filled core feature pl_orbper: 99 missing -> median 4.116\n",
            "Filled core feature pl_rade: 459 missing -> median 10.5\n",
            "Filled core feature st_teff: 148 missing -> median 5789\n",
            "Filled core feature st_rad: 460 missing -> median 1.23\n",
            "Filled core feature st_logg: 759 missing -> median 4.334\n",
            "Filled remaining 85716 numeric NaNs with median\n",
            "\n",
            "Final missing values after imputation: 85716\n",
            "\n",
            "============================================================\n",
            "Final feature count: 98 features\n",
            "Sample count: 7143 samples\n",
            "============================================================\n",
            "============================================================\n",
            "DATA SPLITTING & SCALING\n",
            "============================================================\n",
            "Train samples: 5357\n",
            "Test samples:  1786\n",
            "Scaling complete — numeric features standardized.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 6. MISSING VALUE TREATMENT & FEATURE ENGINEERING ---\n",
        "print(\"=\"*60)\n",
        "print(\"MISSING VALUE TREATMENT & FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "X = df_work.copy()  # operate on working copy\n",
        "\n",
        "# 1️⃣ Boolean flags: if any '_is_limit' columns exist, fill missing with 0\n",
        "bool_cols = [c for c in X.columns if '_is_limit' in c]\n",
        "for col in bool_cols:\n",
        "    before = X[col].isnull().sum()\n",
        "    X[col] = X[col].fillna(0).astype(int)\n",
        "    print(f\"Filled {col}: {before} missing -> 0\")\n",
        "\n",
        "# 2️⃣ Relative errors: fill missing with median\n",
        "err_cols = [c for c in X.columns if '_err_rel' in c]\n",
        "for col in err_cols:\n",
        "    if X[col].isnull().sum() > 0:\n",
        "        median_val = X[col].median()\n",
        "        before = X[col].isnull().sum()\n",
        "        X[col] = X[col].fillna(median_val)\n",
        "        print(f\"Filled {col}: {before} missing -> median {median_val:.6f}\")\n",
        "\n",
        "# 3️⃣ Core numeric features: fill missing with median (prevent dataset shrinkage)\n",
        "core_features = [\n",
        "    'pl_orbper', 'pl_trandurh', 'pl_trandep', 'pl_rade',\n",
        "    'st_teff', 'st_rad', 'st_logg', 'st_tmag'\n",
        "]\n",
        "for col in core_features:\n",
        "    if col in X.columns and X[col].isnull().sum() > 0:\n",
        "        median_val = X[col].median()\n",
        "        before = X[col].isnull().sum()\n",
        "        X[col] = X[col].fillna(median_val)\n",
        "        print(f\"Filled core feature {col}: {before} missing -> median {median_val:.4g}\")\n",
        "\n",
        "# 4️⃣ Replace infinities with NaN\n",
        "X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# 5️⃣ Fill remaining numeric NaNs with median (only numeric columns)\n",
        "numeric_cols = X.select_dtypes(include=np.number).columns\n",
        "if X[numeric_cols].isnull().sum().sum() > 0:\n",
        "    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
        "    print(f\"Filled remaining {X[numeric_cols].isnull().sum().sum()} numeric NaNs with median\")\n",
        "\n",
        "print(f\"\\nFinal missing values after imputation: {X.isnull().sum().sum()}\")\n",
        "\n",
        "# --- FEATURE ENGINEERING ---\n",
        "X['transit_signal_strength'] = X['pl_trandep'] * X['pl_trandurh']\n",
        "X['planet_star_radius_ratio'] = np.sqrt(X['pl_trandep'] / 1e6)\n",
        "X['stellar_density_proxy'] = (X['pl_trandurh'] / X['pl_orbper']) ** 2\n",
        "\n",
        "if 'pl_insol' in X.columns and 'pl_eqt' in X.columns:\n",
        "    X['insol_temp_ratio'] = X['pl_eqt'] / np.sqrt(X['pl_insol'])\n",
        "\n",
        "err_cols_present = [c for c in err_cols if c in X.columns]\n",
        "if err_cols_present:\n",
        "    X['avg_measurement_error'] = X[err_cols_present].mean(axis=1)\n",
        "\n",
        "if 'pl_eqt' in X.columns:\n",
        "    X['is_hot_jupiter'] = (X['pl_eqt'] > 1000).astype(int)\n",
        "\n",
        "X['is_small_planet'] = (X['pl_rade'] < 2.0).astype(int)\n",
        "X['is_bright_star'] = (X['st_tmag'] < 12.0).astype(int)\n",
        "\n",
        "if 'st_pmra' in X.columns and 'st_pmdec' in X.columns:\n",
        "    X['proper_motion_total'] = np.sqrt(X['st_pmra']**2 + X['st_pmdec']**2)\n",
        "\n",
        "X['log_period'] = np.log10(X['pl_orbper'])\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Final feature count: {X.shape[1]} features\")\n",
        "print(f\"Sample count: {X.shape[0]} samples\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRGu_QTzOOFL",
        "outputId": "857269c8-6df3-4dc3-8e62-1c3575e37802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "MISSING VALUE TREATMENT & FEATURE ENGINEERING\n",
            "============================================================\n",
            "Filled core feature pl_orbper: 99 missing -> median 4.116\n",
            "Filled core feature pl_rade: 459 missing -> median 10.5\n",
            "Filled core feature st_teff: 148 missing -> median 5789\n",
            "Filled core feature st_rad: 460 missing -> median 1.23\n",
            "Filled core feature st_logg: 759 missing -> median 4.334\n",
            "Filled remaining 85716 numeric NaNs with median\n",
            "\n",
            "Final missing values after imputation: 85716\n",
            "\n",
            "============================================================\n",
            "Final feature count: 98 features\n",
            "Sample count: 7143 samples\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# --- BASELINE MODEL: RANDOM FOREST ---\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE MODEL: RANDOM FOREST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize classifier\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=None,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "# Train\n",
        "rf_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_clf.predict(X_test_scaled)\n",
        "\n",
        "# Convert target names to strings for report\n",
        "target_names_str = ['Confirmed Planet', 'Planet Candidate', 'False Positive']\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names_str))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCf0SF5pL5jq",
        "outputId": "cfebc28c-de00-4595-fc3c-b00fc95ec5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "BASELINE MODEL: RANDOM FOREST\n",
            "============================================================\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Confirmed Planet       0.80      0.49      0.61       317\n",
            "Planet Candidate       0.77      0.94      0.85      1170\n",
            "  False Positive       0.74      0.42      0.54       299\n",
            "\n",
            "        accuracy                           0.77      1786\n",
            "       macro avg       0.77      0.62      0.66      1786\n",
            "    weighted avg       0.77      0.77      0.75      1786\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 155  156    6]\n",
            " [  33 1100   37]\n",
            " [   6  168  125]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# --- XGBOOST CLASSIFIER WITH HYPERPARAMETER TUNING ---\n",
        "print(\"=\"*60)\n",
        "print(\"XGBOOST CLASSIFIER WITH RANDOMIZED SEARCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize base XGBClassifier\n",
        "xgb_clf = XGBClassifier(\n",
        "    objective='multi:softmax',  # multi-class classification\n",
        "    num_class=3,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 400, 600],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 1, 5],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [1, 1.5, 2.0]\n",
        "}\n",
        "\n",
        "# Randomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,           # number of random combinations to try\n",
        "    scoring='f1_macro',  # focus on macro F1\n",
        "    cv=3,                # 3-fold CV\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit search\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best model\n",
        "best_xgb = random_search.best_estimator_\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "target_names_str = ['Confirmed Planet', 'Planet Candidate', 'False Positive']\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=target_names_str))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "print(cm_xgb)\n"
      ],
      "metadata": {
        "id": "j62x87n0NM3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61bcbf1a-6ee6-42bf-c2cd-6f26e247d3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "XGBOOST CLASSIFIER WITH RANDOMIZED SEARCH\n",
            "============================================================\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:08:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best hyperparameters found:\n",
            "{'subsample': 1.0, 'reg_lambda': 1.5, 'reg_alpha': 0.5, 'n_estimators': 600, 'max_depth': 10, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.6}\n",
            "\n",
            "Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Confirmed Planet       0.77      0.59      0.67       317\n",
            "Planet Candidate       0.81      0.93      0.86      1170\n",
            "  False Positive       0.75      0.51      0.60       299\n",
            "\n",
            "        accuracy                           0.80      1786\n",
            "       macro avg       0.77      0.67      0.71      1786\n",
            "    weighted avg       0.79      0.80      0.79      1786\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 187  120   10]\n",
            " [  45 1083   42]\n",
            " [  11  136  152]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only numeric columns\n",
        "X_train_num = X_train_scaled.select_dtypes(include=np.number)\n",
        "X_test_num = X_test_scaled[X_train_num.columns]\n",
        "\n",
        "# Impute any remaining NaNs with median (should be safe now)\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_num = imputer.fit_transform(X_train_num)\n",
        "X_test_num = imputer.transform(X_test_num)\n"
      ],
      "metadata": {
        "id": "SNubaDdwcu_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b10e17-742b-455b-bf07-e2dd6fdefd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['raerr1' 'raerr2' 'decerr1' 'decerr2' 'pl_insolerr1' 'pl_insolerr2'\n",
            " 'pl_insollim' 'pl_insolsymerr' 'pl_eqterr1' 'pl_eqterr2' 'pl_eqtlim'\n",
            " 'pl_eqtsymerr']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['raerr1' 'raerr2' 'decerr1' 'decerr2' 'pl_insolerr1' 'pl_insolerr2'\n",
            " 'pl_insollim' 'pl_insolsymerr' 'pl_eqterr1' 'pl_eqterr2' 'pl_eqtlim'\n",
            " 'pl_eqtsymerr']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Use numeric, imputed features ---\n",
        "X_train_num = X_train_scaled.select_dtypes(include=np.number)\n",
        "X_test_num = X_test_scaled[X_train_num.columns]\n",
        "\n",
        "# Impute any remaining NaNs with median\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_num = imputer.fit_transform(X_train_num)\n",
        "X_test_num = imputer.transform(X_test_num)\n",
        "\n",
        "# --- Stacking ensemble ---\n",
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Base learners\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=300, max_depth=10, random_state=42)),\n",
        "    ('xgb', XGBClassifier(\n",
        "        objective='multi:softmax', num_class=3, eval_metric='mlogloss',\n",
        "        n_estimators=400, max_depth=7, learning_rate=0.05, random_state=42\n",
        "    )),\n",
        "]\n",
        "\n",
        "# Meta-learner\n",
        "meta_clf = LogisticRegression(multi_class='multinomial', max_iter=1000)\n",
        "\n",
        "# Stacking\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta_clf,\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "# Fit on numeric, imputed data\n",
        "stack_clf.fit(X_train_num, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_stack = stack_clf.predict(X_test_num)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_pred_stack_proba = stack_clf.predict_proba(X_test_num)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nStacked Model Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_stack, target_names=target_names_str))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_stack))\n",
        "\n",
        "# Calculate ROC-AUC\n",
        "# For multi-class, use 'ovo' (one vs one) or 'ovr' (one vs rest)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_stack_proba, multi_class='ovo')\n",
        "print(f\"\\nROC-AUC Score (One vs One): {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "__un4qWJejlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c9654f-a33d-4a4e-e556-0d526e963e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['raerr1' 'raerr2' 'decerr1' 'decerr2' 'pl_insolerr1' 'pl_insolerr2'\n",
            " 'pl_insollim' 'pl_insolsymerr' 'pl_eqterr1' 'pl_eqterr2' 'pl_eqtlim'\n",
            " 'pl_eqtsymerr']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['raerr1' 'raerr2' 'decerr1' 'decerr2' 'pl_insolerr1' 'pl_insolerr2'\n",
            " 'pl_insollim' 'pl_insolsymerr' 'pl_eqterr1' 'pl_eqterr2' 'pl_eqtlim'\n",
            " 'pl_eqtsymerr']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stacked Model Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "Confirmed Planet       0.79      0.60      0.68       317\n",
            "Planet Candidate       0.81      0.93      0.86      1170\n",
            "  False Positive       0.73      0.49      0.59       299\n",
            "\n",
            "        accuracy                           0.80      1786\n",
            "       macro avg       0.78      0.67      0.71      1786\n",
            "    weighted avg       0.79      0.80      0.78      1786\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 191  119    7]\n",
            " [  39 1084   47]\n",
            " [  12  141  146]]\n",
            "\n",
            "ROC-AUC Score (One vs One): 0.8870\n"
          ]
        }
      ]
    }
  ]
}